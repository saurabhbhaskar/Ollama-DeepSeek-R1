# DeepSeek Model with Ollama

This project demonstrates how to run the DeepSeek model locally using Ollama. DeepSeek is an AI model designed for conversational tasks, and this setup allows you to run it on your local machine for testing and experimentation.

## Requirements
- Python 3.x
- Ollama (Local model server)
- Chainlit (UI for chat interactions)

## Setup

1. **Install Ollama**:
   Follow the installation instructions from [Ollama](https://ollama.com/) to get the model running locally.

2. **Install Dependencies**:
   Make sure you have a Python virtual environment set up, and install the necessary Python libraries:

   ```bash
   pip install chainlit ollama
